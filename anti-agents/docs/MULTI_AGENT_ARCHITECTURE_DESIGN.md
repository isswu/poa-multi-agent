# Multi-Agent System Architecture Design for Public Opinion Analysis

**Version**: 1.0  
**Date**: 2025-11-06  
**Status**: Design Proposal

---

## ðŸ“‹ Executive Summary

This document outlines the architecture design for integrating OpenAI Agents Python framework into the Public Opinion Analysis system. The multi-agent system will orchestrate complex workflows including data crawling, content analysis, report generation, and decision support.

### Key Benefits

- **Modularity**: Each agent handles a specific domain with clear responsibilities
- **Scalability**: Agents can be scaled independently based on workload
- **Flexibility**: Easy to add new analysis capabilities or platforms
- **Intelligence**: LLM-powered agents make context-aware decisions
- **Traceability**: Built-in tracing for monitoring and debugging

---

## ðŸ—ï¸ Overall System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Frontend (React)                             â”‚
â”‚                  User Interface & Visualization                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API Gateway (FastAPI)                            â”‚
â”‚                  Task Submission & Results API                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Multi-Agent Orchestrator                         â”‚
â”‚                   (OpenAI Agents Framework)                          â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                   Coordinator Agent                          â”‚  â”‚
â”‚  â”‚           (Workflow orchestration & routing)                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚                                           â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Data Collection   â”‚                    â”‚  Analysis Pipeline â”‚  â”‚
â”‚  â”‚      Agent         â”‚                    â”‚      Agent         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚                                           â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Content Analysis   â”‚                    â”‚  Report Generation â”‚  â”‚
â”‚  â”‚      Agent         â”‚                    â”‚      Agent         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚                                           â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Decision Support  â”‚                    â”‚   Notification     â”‚  â”‚
â”‚  â”‚      Agent         â”‚                    â”‚      Agent         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                â”‚                â”‚
            â–¼                â–¼                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   PostgreSQL   â”‚ â”‚  Redis   â”‚  â”‚ External Tools â”‚
   â”‚   (Storage)    â”‚ â”‚ (Cache)  â”‚  â”‚ (Crawlers, ML) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ¤– Agent Architecture Design

### 1. **Coordinator Agent** (åè°ƒä»£ç†)

**Role**: Master orchestrator that routes tasks to specialized agents

**Responsibilities**:
- Accept analysis tasks from API
- Understand user intent and requirements
- Route tasks to appropriate specialized agents
- Coordinate handoffs between agents
- Aggregate final results
- Handle error recovery and retries

**Tools**:
- `create_crawler_task`: Submit crawling tasks
- `get_task_status`: Monitor task progress
- `query_analysis_results`: Retrieve analysis data

**Handoffs**:
- â†’ Data Collection Agent
- â†’ Analysis Pipeline Agent

**Example Prompt**:
```python
instructions = """
You are the Coordinator Agent for a public opinion analysis system.

Your role is to:
1. Understand user requirements for opinion analysis tasks
2. Determine what data needs to be collected
3. Route data collection to Data Collection Agent
4. Route analysis to Analysis Pipeline Agent
5. Ensure tasks complete successfully
6. Return comprehensive results to users

When users request analysis:
- Clarify scope (platforms, keywords, time range, content types)
- Break down into subtasks
- Coordinate execution
- Handle errors gracefully
"""
```

---

### 2. **Data Collection Agent** (æ•°æ®é‡‡é›†ä»£ç†)

**Role**: Manages multi-platform content crawling

**Responsibilities**:
- Translate analysis requirements into crawler configurations
- Submit crawler tasks (Douyin, Xiaohongshu, Bilibili, etc.)
- Monitor crawler progress and health
- Handle crawler errors and retries
- Ensure data quality and completeness
- Validate collected data

**Tools**:
- `submit_douyin_crawler`: Douyin-specific crawling
- `submit_xhs_crawler`: Xiaohongshu crawling
- `submit_bilibili_crawler`: Bilibili crawling
- `validate_crawler_config`: Validate configurations
- `get_crawler_statistics`: Monitor progress

**Handoffs**:
- â†’ Content Analysis Agent (when data is ready)

**Output Schema**:
```python
class CrawlerResult(BaseModel):
    task_id: str
    platform: Platform
    total_posts: int
    total_accounts: int
    status: TaskStatus
    data_summary: Dict[str, Any]
    execution_time: float
```

**Example Prompt**:
```python
instructions = """
You are the Data Collection Agent specializing in multi-platform social media crawling.

Your capabilities:
- Configure and submit crawler tasks for Douyin, Xiaohongshu, Bilibili
- Monitor crawler execution and health
- Validate data quality
- Handle account rotation and proxy issues
- Resume interrupted tasks

For each crawling request:
1. Determine optimal crawler configuration
2. Select appropriate crawler mode (search/creator/detail)
3. Set reasonable limits and timeouts
4. Monitor execution
5. Validate results before handoff

Always ensure crawled data meets quality standards before proceeding.
"""
```

---

### 3. **Content Analysis Agent** (å†…å®¹åˆ†æžä»£ç†)

**Role**: Performs multi-dimensional content analysis

**Responsibilities**:
- Coordinate multiple analysis tasks
- Route to specialized analyzers
- Aggregate analysis results
- Extract insights and patterns
- Generate structured analysis data

**Sub-Agents (via handoffs)**:
- â†’ Sensitive Content Analyzer
- â†’ Sentiment Analyzer
- â†’ Topic Analyzer
- â†’ Trend Analyzer

**Tools**:
- `analyze_sensitive_content`: NSFW/violence detection
- `analyze_sentiment`: Emotion and attitude analysis
- `extract_topics`: Topic modeling and extraction
- `detect_trends`: Trend and pattern detection
- `analyze_engagement`: Engagement metrics analysis

**Output Schema**:
```python
class AnalysisResult(BaseModel):
    post_id: str
    platform: Platform
    sensitive_content_score: Optional[SensitiveContentResult]
    sentiment_score: Optional[SentimentResult]
    topics: List[str]
    trends: List[TrendIndicator]
    engagement_analysis: EngagementMetrics
    risk_level: RiskLevel
```

**Example Prompt**:
```python
instructions = """
You are the Content Analysis Agent coordinating multi-dimensional analysis.

Your role:
1. Analyze collected social media content comprehensively
2. Detect sensitive content (NSFW, violence, illegal)
3. Perform sentiment analysis (positive, negative, neutral)
4. Extract topics and themes
5. Identify trends and patterns
6. Calculate engagement metrics
7. Assess risk levels

For each content batch:
- Prioritize sensitive content detection
- Perform parallel analysis where possible
- Aggregate results systematically
- Flag high-risk content immediately
- Provide actionable insights
"""
```

---

### 4. **Sensitive Content Analyzer** (æ•æ„Ÿå†…å®¹åˆ†æžå­ä»£ç†)

**Role**: Specialized agent for NSFW and harmful content detection

**Responsibilities**:
- Video frame extraction and analysis
- NSFW content detection (porn, violence)
- Political sensitive content detection
- Illegal content identification
- Generate violation reports

**Tools**:
- `detect_porn_content`: Pornographic content detection
- `detect_violence_content`: Violence detection
- `detect_political_content`: Political sensitivity check
- `extract_video_frames`: Frame sampling from videos

**Output Schema**:
```python
class SensitiveContentResult(BaseModel):
    video_id: str
    has_violation: bool
    violation_types: List[ViolationType]
    confidence_scores: Dict[ViolationType, float]
    violation_segments: List[TimeRange]
    recommendation: str  # block, review, approve
```

---

### 5. **Sentiment Analyzer** (æƒ…æ„Ÿåˆ†æžå­ä»£ç†)

**Role**: Analyzes emotional tone and public sentiment

**Responsibilities**:
- Text sentiment classification
- Emotion detection (joy, anger, sadness, etc.)
- Attitude analysis (supportive, critical, neutral)
- Sentiment trend over time
- Audience reaction analysis

**Tools**:
- `analyze_text_sentiment`: NLP-based sentiment analysis
- `detect_emotions`: Multi-label emotion detection
- `analyze_comment_sentiment`: Comment section analysis
- `calculate_sentiment_score`: Aggregate sentiment scoring

**Output Schema**:
```python
class SentimentResult(BaseModel):
    post_id: str
    overall_sentiment: Sentiment  # positive, negative, neutral
    sentiment_score: float  # -1.0 to 1.0
    emotions: Dict[Emotion, float]
    attitude: Attitude  # supportive, critical, neutral
    confidence: float
```

---

### 6. **Topic Analyzer** (ä¸»é¢˜åˆ†æžå­ä»£ç†)

**Role**: Extracts topics and themes from content

**Responsibilities**:
- Topic modeling and extraction
- Keyword extraction
- Theme clustering
- Topic trend analysis
- Cross-platform topic tracking

**Tools**:
- `extract_keywords`: Important keyword extraction
- `identify_topics`: LDA/BERTopic modeling
- `cluster_themes`: Content clustering
- `track_topic_spread`: Cross-platform tracking

**Output Schema**:
```python
class TopicResult(BaseModel):
    post_id: str
    primary_topic: str
    related_topics: List[str]
    keywords: List[Tuple[str, float]]  # (keyword, importance)
    topic_category: str
    topic_cluster_id: Optional[int]
```

---

### 7. **Trend Analyzer** (è¶‹åŠ¿åˆ†æžå­ä»£ç†)

**Role**: Identifies patterns and emerging trends

**Responsibilities**:
- Viral content detection
- Trend forecasting
- Anomaly detection
- Influencer identification
- Spread pattern analysis

**Tools**:
- `detect_viral_content`: Viral content identification
- `forecast_trend`: Trend prediction
- `detect_anomalies`: Unusual pattern detection
- `identify_influencers`: Key opinion leader detection

**Output Schema**:
```python
class TrendResult(BaseModel):
    trend_id: str
    trend_name: str
    trend_type: TrendType  # rising, declining, stable
    growth_rate: float
    related_posts: List[str]
    influencers: List[str]
    forecast: TrendForecast
```

---

### 8. **Report Generation Agent** (æŠ¥å‘Šç”Ÿæˆä»£ç†)

**Role**: Creates comprehensive analysis reports

**Responsibilities**:
- Aggregate analysis results
- Generate executive summaries
- Create visualizations
- Produce multi-format reports (PDF, HTML, JSON)
- Customize reports by audience

**Tools**:
- `aggregate_analysis_data`: Combine all analysis results
- `generate_summary`: Create executive summary
- `create_visualizations`: Charts and graphs
- `format_report`: Multi-format output
- `generate_recommendations`: Actionable insights

**Output Schema**:
```python
class AnalysisReport(BaseModel):
    report_id: str
    task_id: str
    generated_at: datetime
    
    executive_summary: str
    data_overview: DataOverview
    
    sensitive_content_summary: SensitiveContentSummary
    sentiment_analysis: SentimentSummary
    topic_analysis: TopicSummary
    trend_analysis: TrendSummary
    
    risk_assessment: RiskAssessment
    recommendations: List[Recommendation]
    
    visualizations: List[Visualization]
    raw_data_references: Dict[str, str]
```

**Example Prompt**:
```python
instructions = """
You are the Report Generation Agent creating comprehensive analysis reports.

Your responsibilities:
1. Aggregate data from all analysis agents
2. Generate clear, actionable executive summaries
3. Present findings with appropriate visualizations
4. Assess overall risk levels
5. Provide strategic recommendations

Report structure:
- Executive Summary: Key findings and recommendations
- Data Overview: Scope and coverage statistics
- Sensitive Content: Violations and risks
- Sentiment Analysis: Public opinion trends
- Topic Analysis: Main themes and discussions
- Trend Analysis: Emerging patterns
- Risk Assessment: Overall risk evaluation
- Recommendations: Actionable next steps

Tailor language and detail to audience (executive, analyst, operator).
"""
```

---

### 9. **Decision Support Agent** (è¾…åŠ©å†³ç­–ä»£ç†)

**Role**: Provides strategic recommendations and action plans

**Responsibilities**:
- Interpret analysis results
- Generate strategic recommendations
- Risk prioritization
- Action plan generation
- Alert high-priority issues
- Suggest intervention strategies

**Tools**:
- `assess_risks`: Risk level evaluation
- `generate_action_plan`: Step-by-step action plans
- `prioritize_issues`: Issue prioritization
- `suggest_interventions`: Intervention strategies
- `estimate_impact`: Impact estimation

**Output Schema**:
```python
class DecisionSupport(BaseModel):
    task_id: str
    overall_risk_level: RiskLevel
    
    priority_issues: List[PriorityIssue]
    recommended_actions: List[Action]
    
    risk_mitigation_strategies: List[Strategy]
    resource_allocation_suggestions: Dict[str, Any]
    
    timeline: Timeline
    success_metrics: List[Metric]
```

**Example Prompt**:
```python
instructions = """
You are the Decision Support Agent providing strategic guidance.

Your role:
1. Analyze all findings comprehensively
2. Assess overall risk and urgency
3. Prioritize issues requiring immediate attention
4. Generate actionable recommendations
5. Suggest intervention strategies
6. Estimate resource requirements

For each analysis:
- Identify critical risks
- Recommend immediate actions for high-risk items
- Suggest preventive measures
- Provide implementation timelines
- Define success metrics

Be specific, actionable, and practical in your recommendations.
"""
```

---

### 10. **Notification Agent** (é€šçŸ¥ä»£ç†)

**Role**: Manages alerts and notifications

**Responsibilities**:
- Monitor for high-priority events
- Send real-time alerts
- Escalate critical issues
- Notify stakeholders
- Generate alert summaries

**Tools**:
- `send_alert`: Send immediate alerts
- `escalate_issue`: Escalate to higher authority
- `notify_stakeholders`: Stakeholder notifications
- `create_alert_summary`: Alert summary generation

---

## ðŸ”„ Workflow Examples

### Workflow 1: Keyword-Based Opinion Analysis

```python
from agents import Agent, Runner

# User request
user_request = """
åˆ†æžæŠ–éŸ³å¹³å°ä¸Šå…³äºŽ"äººå·¥æ™ºèƒ½"çš„èˆ†æƒ…ï¼Œ
åŒ…æ‹¬å†…å®¹åˆ†æžã€æƒ…æ„Ÿå€¾å‘ã€çƒ­ç‚¹è¯é¢˜å’Œé£Žé™©è¯„ä¼°ã€‚
æ—¶é—´èŒƒå›´ï¼šæœ€è¿‘7å¤©ï¼Œåˆ†æžé‡ï¼š500æ¡ã€‚
"""

# Run coordinator
result = await Runner.run(coordinator_agent, input=user_request)

# Internal flow:
# 1. Coordinator â†’ Data Collection Agent
#    - Submit Douyin search crawler (keyword: "äººå·¥æ™ºèƒ½", max: 500)
# 2. Data Collection Agent â†’ Content Analysis Agent
#    - Pass collected posts for analysis
# 3. Content Analysis Agent â†’ Sub-analyzers (parallel)
#    - Sensitive Content Analyzer
#    - Sentiment Analyzer
#    - Topic Analyzer
#    - Trend Analyzer
# 4. Analysis Results â†’ Report Generation Agent
#    - Generate comprehensive report
# 5. Report â†’ Decision Support Agent
#    - Generate recommendations
# 6. Final Report â†’ Coordinator â†’ User
```

---

### Workflow 2: Creator Monitoring

```python
user_request = """
ç›‘æŽ§ä»¥ä¸‹æŠ–éŸ³åˆ›ä½œè€…çš„å†…å®¹ï¼š
- MS4wLjABAAAA... (creator_id_1)
- MS4wLjABAAAA... (creator_id_2)

æ£€æµ‹æ•æ„Ÿå†…å®¹ï¼Œåˆ†æžè§‚ä¼—ååº”ï¼Œè¯„ä¼°é£Žé™©ã€‚
"""

# Internal flow:
# 1. Coordinator â†’ Data Collection Agent
#    - Submit creator-mode crawler for each creator
# 2. Data Collection Agent â†’ Sensitive Content Analyzer (priority)
#    - Immediate sensitive content check
# 3. If violations found â†’ Notification Agent
#    - Send real-time alert
# 4. Parallel: Sentiment + Engagement Analysis
# 5. Report Generation â†’ Decision Support
# 6. Return results with action plan
```

---

### Workflow 3: Cross-Platform Trend Analysis

```python
user_request = """
åˆ†æž"åŒåä¸€"è¯é¢˜åœ¨æŠ–éŸ³ã€å°çº¢ä¹¦ã€Bç«™çš„ä¼ æ’­è¶‹åŠ¿ã€‚
å¯¹æ¯”å„å¹³å°ç”¨æˆ·æƒ…æ„Ÿå’Œè®¨è®ºçƒ­ç‚¹ã€‚
"""

# Internal flow:
# 1. Coordinator â†’ Data Collection Agent
#    - Submit parallel crawlers for all 3 platforms
# 2. Data Collection Agent â†’ Content Analysis Agent
#    - Analyze content from each platform
# 3. Content Analysis â†’ Topic Analyzer
#    - Extract platform-specific topics
# 4. Content Analysis â†’ Sentiment Analyzer
#    - Compare sentiment across platforms
# 5. Content Analysis â†’ Trend Analyzer
#    - Cross-platform trend comparison
# 6. Report Generation Agent
#    - Generate comparative analysis report
# 7. Decision Support Agent
#    - Strategic recommendations per platform
```

---

## ðŸ› ï¸ Implementation Architecture

### Technology Stack

```yaml
Multi-Agent Framework:
  - openai-agents: Core agent framework
  - openai: LLM API (GPT-4)
  - pydantic: Data validation

Backend Services:
  - FastAPI: API gateway
  - Celery: Async task execution
  - SQLAlchemy: Database ORM
  - Redis: Caching & message broker

Existing Modules:
  - poa-media-crawler: Data collection
  - sensitive-content-detection: NSFW detection
  
Storage:
  - PostgreSQL: Primary database
  - Redis: Cache & sessions
  
Monitoring:
  - Logfire/AgentOps: Agent tracing
  - Flower: Celery monitoring
```

---

### Project Structure

```
shine/
â”œâ”€â”€ poa-multi-agent/              # New: Multi-Agent Module
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”‚   â”œâ”€â”€ coordinator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ data_collection.py
â”‚   â”‚   â”‚   â”œâ”€â”€ content_analysis.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ sensitive_content.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ sentiment.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ topic.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ trend.py
â”‚   â”‚   â”‚   â”œâ”€â”€ report_generation.py
â”‚   â”‚   â”‚   â”œâ”€â”€ decision_support.py
â”‚   â”‚   â”‚   â””â”€â”€ notification.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ tools/                # Agent tools
â”‚   â”‚   â”‚   â”œâ”€â”€ crawler_tools.py
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis_tools.py
â”‚   â”‚   â”‚   â”œâ”€â”€ report_tools.py
â”‚   â”‚   â”‚   â””â”€â”€ notification_tools.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ workflows/            # Pre-defined workflows
â”‚   â”‚   â”‚   â”œâ”€â”€ keyword_analysis.py
â”‚   â”‚   â”‚   â”œâ”€â”€ creator_monitoring.py
â”‚   â”‚   â”‚   â””â”€â”€ trend_analysis.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ schemas/              # Data models
â”‚   â”‚   â”‚   â”œâ”€â”€ requests.py
â”‚   â”‚   â”‚   â”œâ”€â”€ responses.py
â”‚   â”‚   â”‚   â””â”€â”€ outputs.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ api/                  # FastAPI routes
â”‚   â”‚   â”‚   â””â”€â”€ routers/
â”‚   â”‚   â”‚       â”œâ”€â”€ analysis.py
â”‚   â”‚   â”‚       â””â”€â”€ workflows.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ services/             # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ agent_runner.py
â”‚   â”‚   â”‚   â””â”€â”€ session_manager.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ poa-media-crawler/            # Existing: Crawler module
â”œâ”€â”€ sensitive-content-detection/  # Existing: Detection module
â””â”€â”€ ...
```

---

### Database Schema Extensions

```sql
-- Agent execution tracking
CREATE TABLE agent_executions (
    id SERIAL PRIMARY KEY,
    session_id VARCHAR(255) NOT NULL,
    agent_name VARCHAR(100) NOT NULL,
    task_type VARCHAR(50),
    status VARCHAR(20),
    input_data JSONB,
    output_data JSONB,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    error_message TEXT,
    trace_id VARCHAR(255)
);

-- Analysis results
CREATE TABLE analysis_results (
    id SERIAL PRIMARY KEY,
    task_id VARCHAR(255) NOT NULL,
    post_id VARCHAR(255),
    platform VARCHAR(50),
    
    sensitive_content_result JSONB,
    sentiment_result JSONB,
    topic_result JSONB,
    trend_result JSONB,
    
    risk_level VARCHAR(20),
    analyzed_at TIMESTAMP,
    
    FOREIGN KEY (post_id) REFERENCES media_posts(id)
);

-- Generated reports
CREATE TABLE analysis_reports (
    id SERIAL PRIMARY KEY,
    report_id VARCHAR(255) UNIQUE NOT NULL,
    task_id VARCHAR(255),
    
    report_type VARCHAR(50),
    executive_summary TEXT,
    report_data JSONB,
    
    generated_at TIMESTAMP,
    generated_by VARCHAR(100)
);

-- Decision recommendations
CREATE TABLE decision_recommendations (
    id SERIAL PRIMARY KEY,
    task_id VARCHAR(255),
    report_id VARCHAR(255),
    
    risk_level VARCHAR(20),
    priority_issues JSONB,
    recommended_actions JSONB,
    
    created_at TIMESTAMP,
    
    FOREIGN KEY (report_id) REFERENCES analysis_reports(report_id)
);
```

---

## ðŸ” Security & Access Control

### API Authentication

```python
# API key authentication for agent services
from fastapi import Security, HTTPException
from fastapi.security.api_key import APIKeyHeader

api_key_header = APIKeyHeader(name="X-API-Key")

async def verify_api_key(api_key: str = Security(api_key_header)):
    if api_key not in settings.valid_api_keys:
        raise HTTPException(status_code=403, detail="Invalid API key")
    return api_key
```

### Rate Limiting

```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/api/v1/analysis")
@limiter.limit("10/minute")
async def create_analysis_task(request: AnalysisRequest):
    ...
```

---

## ðŸ“Š Monitoring & Observability

### Agent Tracing

```python
from agents.tracing import AgentTracer

tracer = AgentTracer(
    backend="logfire",  # or "agentops", "braintrust"
    project="public-opinion-analysis"
)

# Automatic tracing for all agent runs
Runner.run(
    coordinator_agent,
    input=user_request,
    tracer=tracer
)
```

### Metrics

```python
# Key metrics to monitor
metrics = {
    "agent_execution_time": "Histogram",
    "agent_handoff_count": "Counter",
    "agent_error_rate": "Gauge",
    "task_completion_rate": "Gauge",
    "analysis_throughput": "Counter",
}
```

---

## ðŸš€ Deployment Strategy

### Phase 1: Core Infrastructure (Week 1-2)
- [ ] Set up `poa-multi-agent` module
- [ ] Implement Coordinator Agent
- [ ] Implement Data Collection Agent
- [ ] Integrate with existing crawler module
- [ ] Basic API endpoints

### Phase 2: Analysis Agents (Week 3-4)
- [ ] Implement Content Analysis Agent
- [ ] Integrate Sensitive Content Analyzer
- [ ] Implement Sentiment Analyzer
- [ ] Implement Topic Analyzer
- [ ] Implement Trend Analyzer

### Phase 3: Reporting & Decision (Week 5-6)
- [ ] Implement Report Generation Agent
- [ ] Implement Decision Support Agent
- [ ] Implement Notification Agent
- [ ] Complete workflow examples

### Phase 4: Testing & Optimization (Week 7-8)
- [ ] Integration testing
- [ ] Performance optimization
- [ ] Agent prompt tuning
- [ ] Documentation

### Phase 5: Production Deployment (Week 9-10)
- [ ] Production environment setup
- [ ] Monitoring & alerting
- [ ] User training
- [ ] Go-live

---

## ðŸ’° Cost Estimation

### LLM API Costs (OpenAI GPT-4)

```python
# Assuming GPT-4 Turbo pricing: $10/1M input tokens, $30/1M output tokens

# Per analysis task (average):
estimated_costs = {
    "coordinator": {"input": 2000, "output": 500},     # $0.035
    "data_collection": {"input": 1500, "output": 300}, # $0.024
    "content_analysis": {"input": 5000, "output": 2000}, # $0.110
    "report_generation": {"input": 8000, "output": 3000}, # $0.170
    "decision_support": {"input": 6000, "output": 2000}, # $0.120
}

# Total per task: ~$0.46
# For 1000 tasks/day: ~$460/day = ~$14,000/month
```

### Infrastructure Costs

```yaml
Compute:
  - Agents Service: $200/month (2 CPU, 4GB RAM)
  - Existing Services: Current costs
  
Storage:
  - PostgreSQL: Current costs
  - Redis: Current costs
  
LLM API:
  - OpenAI GPT-4: ~$14,000/month (1000 tasks/day)
  - Alternative: GPT-4-mini ($2/M in, $6/M out) â†’ ~$4,200/month

Total Additional: ~$4,400 - $14,200/month (depending on model choice)
```

---

## ðŸŽ¯ Success Metrics

### Performance Metrics
- **Task Completion Rate**: > 95%
- **Average Task Duration**: < 10 minutes
- **Agent Error Rate**: < 5%
- **API Response Time**: < 2 seconds

### Quality Metrics
- **Analysis Accuracy**: > 90% (vs. human labeling)
- **False Positive Rate**: < 10%
- **Report Usefulness Score**: > 4/5 (user feedback)

### Business Metrics
- **Daily Active Tasks**: Target baseline + 50%
- **User Satisfaction**: > 4.5/5
- **Cost per Analysis**: < $0.50
- **ROI**: Positive within 6 months

---

## ðŸ”® Future Enhancements

### Phase 6: Advanced Features
- **Multi-modal Analysis**: Image, audio, video combined analysis
- **Real-time Streaming**: Live opinion monitoring
- **Predictive Analytics**: Trend forecasting with ML models
- **Custom Agent Training**: Domain-specific agent fine-tuning
- **Multi-language Support**: International platform analysis

### Phase 7: AI Features
- **Auto-generated Alerts**: Smart anomaly detection
- **Natural Language Queries**: "Show me negative sentiment trending topics"
- **Interactive Dashboards**: Conversational BI
- **Recommendation Engine**: Proactive suggestions

---

## ðŸ“ Appendix

### A. Agent Prompt Templates

See `/templates/prompts/` directory for detailed prompt templates.

### B. API Reference

See OpenAPI documentation at `/api/docs`

### C. Error Handling Guide

See `/docs/ERROR_HANDLING.md`

### D. Performance Tuning Guide

See `/docs/PERFORMANCE_TUNING.md`

---

## âœ… Next Steps

1. **Review this proposal** with team
2. **Estimate resource requirements** (time, personnel, budget)
3. **Prioritize features** (MVP vs. nice-to-have)
4. **Approve/modify design**
5. **Begin Phase 1 implementation**

---

**Document Prepared By**: AI Assistant  
**For Review By**: Development Team  
**Approval Required**: Project Manager, Tech Lead, Stakeholders

